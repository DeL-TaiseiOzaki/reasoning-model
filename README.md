<p align="center">
  <img src="assets/top_image.png" alt="Top Image">
</p>

# MCTS-Integrated Causal LM Generation

<div align="center">

<img src="https://img.shields.io/badge/License-Apache%202.0-green.svg" alt="License">
<img src="https://img.shields.io/github/stars/Hajime-Y/reasoning-model?color=yellow" alt="Stars">
<img src="https://img.shields.io/github/issues/Hajime-Y/reasoning-model?color=red" alt="Issues">

:octocat: [**Github**](https://github.com/Hajime-Y/reasoning-model)  ğŸ¤—  [**Hugging Face**](https://huggingface.co/collections/HachiML/reasoning-model-675ff6e972e4137892ff396b) ğŸ“  [**Blog**](https://note.com/hatti8/n/n547d03c6a5c9) ğŸ§‘â€ğŸ’» [**Model**](https://huggingface.co/HachiML/QwQ-CoT-0.5B-JA) ğŸ—‚ï¸  [**Data**](https://huggingface.co/datasets/HachiML/OpenMathInstruct-2-CoT-JA)

</div>

#

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€Hugging Face Transformersã® `AutoModelForCausalLM` ã‚’æ‹¡å¼µã—ã€Monte Carlo Tree Search (MCTS) ã‚’ç”¨ã„ãŸã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®æ¢ç´¢ã«åŸºã¥ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ ã“ã‚Œã«ã‚ˆã£ã¦ã€CoTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æ¨è«–èƒ½åŠ›ã‚’æ›´ã«å‘ä¸Šã•ã›ã¾ã™ã€‚ 
 - `ReasoningModelForCausalLM` ã¯ãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã‚’ç›´æ¥å—ã‘å–ã‚Šã€MCTSã«ã‚ˆã‚‹æ¢ç´¢ã‚’è¡Œã„ã¾ã™ã€‚
 - ãƒ¢ãƒ‡ãƒ«ã¯`.generate`å¾Œã«ã€ä»¥ä¸‹ã‚’è¿”ã—ã¾ã™ã€‚
    - æœ€çµ‚æ¨è«–çµæœã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ
    - æ¢ç´¢ã—ãŸæœ¨æ§‹é€ 
 - `tree_utils` ã® `print_tree_with_best_path` ã‚’ä½¿ã†ã¨ã€ãƒ„ãƒªãƒ¼æ§‹é€ ãŒé¸æŠã•ã‚ŒãŸãƒ‘ã‚¹ã¨å…±ã«å¯è¦–åŒ–ã§ãã¾ã™ã€‚

## ğŸ¤– å¯¾è±¡ãƒ¢ãƒ‡ãƒ«
ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã§å‹•ãã“ã¨ã¯ç¢ºèªæ¸ˆã¿ã€‚åŸºæœ¬çš„ã«ã€Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® `AutoModelForCausalLM` ã§å‹•ã‹ã›ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯GGUFå«ã‚ã¦å‹•ã‹ã›ã‚‹ã¯ãšã§ã™ã€‚
 - [HachiML/QwQ-CoT-0.5B-JA](https://huggingface.co/HachiML/QwQ-CoT-0.5B-JA)
 - [Qwen/QwQ-32B-Preview](https://huggingface.co/Qwen/QwQ-32B-Preview)
 - [AIDC-AI/Marco-o1](https://huggingface.co/AIDC-AI/Marco-o1)
 - [Kendamarron/llm-jp-3-3.7b-o1-v0.1](https://huggingface.co/Kendamarron/llm-jp-3-3.7b-o1-v0.1)
 - [Kendamarron/Qwen2.5-7B-o1-ja-v0.1](https://huggingface.co/Kendamarron/Qwen2.5-7B-o1-ja-v0.1)
 - [mmnga/Marco-o1-gguf](https://huggingface.co/mmnga/Marco-o1-gguf)

âš ï¸ Limitations: ç²¾åº¦å‘ä¸Šã®ç¢ºèªã¯ã€è‡ªèº«ã§å­¦ç¿’ã•ã›ãŸCoTãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ [HachiML/QwQ-CoT-0.5B-JA](https://huggingface.co/HachiML/QwQ-CoT-0.5B-JA) ã§ã®ã¿è¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚Marco-o1ã€QwQ-32B-Previewãªã©ã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã§ã®ç²¾åº¦å‘ä¸Šã¯ã‚ãã¾ã§ä¿è¨¼ã•ã‚Œã¾ã›ã‚“ã€‚

## ğŸ“ ä½¿ã„æ–¹
### æº–å‚™
```
# GitHubãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/Hajime-Y/reasoning-model.git
cd reasoning-model

# poetryåˆ©ç”¨
poetry install

# pipåˆ©ç”¨ï¼ˆéæ¨å¥¨ï¼‰
pip install transformers torch numpy gguf
```

### ãƒ‘ã‚¹è¿½åŠ ï¼ˆGoogle Colabä½¿ç”¨æ™‚ï¼‰
```
# Pythonãƒ‘ã‚¹ã«ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
import sys
sys.path.append('.')  # reasoning_model.py ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã„ã‚‹ãŸã‚
```

### ç”Ÿæˆ
```
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from reasoning_model import ReasoningModelForCausalLM
from tree_utils import print_tree_with_best_path
from transformers import AutoTokenizer

# tokenizerã¨modelã®æº–å‚™
model_name = "HachiML/QwQ-CoT-0.5B-JA"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = ReasoningModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

system_prompt = "You are a helpful and harmless assistant. You should think step-by-step."  # å›ºå®šã‚’æ¨å¥¨
prompt = "231 + 65*6 = ?"
messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": prompt}
]

# chat_templateã¨tokenize
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# MCTSã‚’ç”¨ã„ã¦ç”Ÿæˆï¼ˆGoogle Colabã®T4ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§1åˆ†ç¨‹åº¦ã‹ã‹ã‚‹ï¼‰
final_tokens, final_node = model.generate(
    **model_inputs,
    iterations_per_step=5,      # 1æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®æ¢ç´¢ã«ä½•å›ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ã‹ã€‚é•·ã„ã»ã©ç²¾åº¦ãŒé«˜ã¾ã‚‹å¯èƒ½æ€§ã¯ã‚ã‚‹ãŒã€æ¨è«–æ™‚é–“ãŒä¼¸ã³ã‚‹ã€‚
    max_iterations=15,          # æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸Šé™: 0.5Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãã“ã¾ã§é•·ã„ã‚¹ãƒ†ãƒƒãƒ—ã®æ¨è«–ã¯ã§ããªã„ãŸã‚10~15ãã‚‰ã„ãŒå¦¥å½“ã€‚
    mini_step_size=32,          # mini-step: 32tokensã€‚Step as Actionæˆ¦ç•¥ã‚’æ¡ç”¨ã™ã‚‹å ´åˆã€ã“ã“ã‚’512ãªã©å¤§ããªæ•°å­—ã«ã™ã‚‹ã€‚ï¼ˆå®Ÿè¡Œæ™‚é–“ãŒä¼¸ã³ã‚‹ãŸã‚éæ¨å¥¨ï¼‰
    expand_threshold=0,         # ãƒãƒ¼ãƒ‰ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«å¿…è¦ãªãã®ãƒãƒ¼ãƒ‰ã®è¨ªå•å›æ•°ã®é–¾å€¤ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®0ã§ã€æ‹¡å¼µã«ã¯1å›ä»¥ä¸Šã®è¨ªå•ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚åŸºæœ¬ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è‰¯ã„ã€‚
    step_separator_ids=None,    # Reasoning Action Strategyã§Step as Actionã‚’æ¡ç”¨ã™ã‚‹ã¨ãã®åŒºåˆ‡ã‚Šã¨ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®IDãƒªã‚¹ãƒˆã€‚Noneã§ãƒ¢ãƒ‡ãƒ«Configã®å€¤ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã€å¤‰æ›´ã¯éæ¨å¥¨ã€‚Step as Actionä¸æ¡ç”¨æ™‚ã«ã¯[]ã‚’è¨­å®šã™ã‚‹ã€‚
)

# çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
final_text = tokenizer.decode(final_tokens, skip_special_tokens=True)
print("=== æœ€çµ‚ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆ ===")
print(final_text)
```

### ãƒ„ãƒªãƒ¼æ§‹é€ ã®ç¢ºèª
```
# ãƒ„ãƒªãƒ¼æ§‹é€ è¡¨ç¤º
print("=== ãƒ„ãƒªãƒ¼æ§‹é€  ===")
print_tree_with_best_path(final_node, tokenizer)
```

## ä½¿ã„æ–¹(é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«)
é€šå¸¸ã®Transformersã®AutoModelForCausalLMã¨åŒæ§˜ã®ä½¿ã„æ–¹ã§ã€é‡å­åŒ–ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚‚åˆ©ç”¨å¯èƒ½ã€‚
ä»¥ä¸‹ã¯GGUFã®ä¾‹ã€‚
```
# ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
model_name = "mmnga/Marco-o1-gguf"
file_name = "Marco-o1-Q6_K.gguf"

# Tokenizerã®æº–å‚™
tokenizer = AutoTokenizer.from_pretrained(model_name, gguf_file=file_name)

# ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
model = ReasoningModelForCausalLM.from_pretrained(model_name, gguf_file=file_name)
```

## ğŸš¨ æ³¨æ„äº‹é …
æœªæ¤œè¨¼ã ãŒã€ãŠãã‚‰ã `Qwen/QwQ-32B-Preview` ã‚„ `AIDC-AI/Marco-o1` ã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚‚å®Ÿè¡Œå¯èƒ½ã€‚
ãŸã ã—ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯configã«æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã®`step_separator_ids`ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€Step as Actionã‚’æ¡ç”¨ã™ã‚‹ã¨ãã¯generateå®Ÿè¡Œå‰ã«configã«ç™»éŒ²ãŒå¿…è¦ã€‚

```
# step_separator_idsã®ç™»éŒ²
step_separators = ["\n\n"]
step_separator_ids = [tokenizer.encode(step_separator, add_special_tokens=False)[0] for step_separator in step_separators]
model.config.step_separator_ids = step_separator_ids
```

ã‚‚ã—ãã¯ã€generateæ™‚ã«generateã«step_separator_idsã‚’è¨­å®šã™ã‚‹ã€‚
```
# ç”Ÿæˆæ™‚ã«step_separator_idsã‚’è¨­å®š
step_separators = ["\n\n"]
step_separator_ids = [tokenizer.encode(step_separator, add_special_tokens=False)[0] for step_separator in step_separators]

# MCTSã‚’ç”¨ã„ã¦ç”Ÿæˆ
final_tokens, final_node = model.generate(
    **model_inputs,
    step_separator_ids=step_separator_ids,
)
```

## ğŸ™ I NEED YOUR CONTRIBUTION

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯å®Ÿé¨“çš„ãªæ®µéšã«ã‚ã‚Šã€æ”¹å–„ã®ãŸã‚ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚„Contributionã‚’å‹Ÿé›†ä¸­ã§ã™ï¼
æ°—ã«ãªã£ãŸã“ã¨ãŒã‚ã‚Œã°ã€Issueã‚„Pull Requestã‚’ãŠæ°—è»½ã«ãŠå¯„ã›ãã ã•ã„ğŸ™Œ
